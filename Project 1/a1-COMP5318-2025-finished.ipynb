{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group number: group-set1 200\n",
    "##### Student 1 SID: 540799107\n",
    "##### Student 2 SID:  520534353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "from sklearn.model_selection import StratifiedKFold,cross_val_score, train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "rice_data = pd.read_csv('rice-final2.csv', na_values='?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Area               1396 non-null   float64\n",
      " 1   Perimiter          1396 non-null   float64\n",
      " 2   Major_Axis_Length  1395 non-null   float64\n",
      " 3   Minor_Axis_Length  1397 non-null   float64\n",
      " 4   Eccentricity       1394 non-null   float64\n",
      " 5   Convex_Area        1395 non-null   float64\n",
      " 6   Extent             1398 non-null   float64\n",
      " 7   class              1400 non-null   object \n",
      "dtypes: float64(7), object(1)\n",
      "memory usage: 87.6+ KB\n"
     ]
    }
   ],
   "source": [
    "rice_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class_column = rice_data.columns[-1]\n",
    "feature_columns = rice_data.columns[:-1]\n",
    "\n",
    "# Store all numeric columns into list\n",
    "numeric_columns = rice_data[feature_columns].select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Fill missing value with mean value within each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "rice_data[numeric_columns] = imputer.fit_transform(rice_data[numeric_columns])\n",
    "\n",
    "#Normalize all features columns into (0,1)\n",
    "scaler = MinMaxScaler()\n",
    "rice_data[numeric_columns] = scaler.fit_transform(rice_data[numeric_columns])\n",
    "\n",
    "# Repalce class1,class2 with 0,1 \n",
    "rice_data[class_column] = rice_data[class_column].replace({'class1': 0, 'class2': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4628,0.5406,0.5113,0.4803,0.7380,0.4699,0.1196,1\n",
      "0.4900,0.5547,0.5266,0.5018,0.7319,0.4926,0.8030,1\n",
      "0.6109,0.6847,0.6707,0.5409,0.8032,0.6253,0.1185,0\n",
      "0.6466,0.6930,0.6677,0.5961,0.7601,0.6467,0.2669,0\n",
      "0.6712,0.6233,0.4755,0.8293,0.3721,0.6803,0.4211,1\n",
      "0.2634,0.2932,0.2414,0.4127,0.5521,0.2752,0.2825,1\n",
      "0.8175,0.9501,0.9515,0.5925,0.9245,0.8162,0.0000,0\n",
      "0.3174,0.3588,0.3601,0.3908,0.6921,0.3261,0.8510,1\n",
      "0.3130,0.3050,0.2150,0.5189,0.3974,0.3159,0.4570,1\n",
      "0.5120,0.5237,0.4409,0.6235,0.5460,0.5111,0.3155,1\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "\n",
    "\n",
    "#4.Print the first 10 rows of the pre-processed dataset.\n",
    "X = rice_data[numeric_columns].values\n",
    "y = rice_data[class_column].values\n",
    "\n",
    "print_data(X, y, n_rows=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    #Initializes a logistic regression classifier\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    \n",
    "    #Conduct cross-validation\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    \n",
    "    #Reture the mean results of cross-validation\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    #Initializes a Naïve Bayes classifier\n",
    "    clf = GaussianNB()\n",
    "    \n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    #Initializes a Decision Tre classifier and using entropy as criterion\n",
    "    clf = DecisionTreeClassifier(random_state=0,criterion='entropy')\n",
    "    \n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    \n",
    "    base_dt = DecisionTreeClassifier(random_state=0, criterion='entropy', max_depth=max_depth)\n",
    "\n",
    "    bag_clf = BaggingClassifier(estimator=base_dt, \n",
    "                                n_estimators=n_estimators, \n",
    "                                max_samples=max_samples, \n",
    "                                random_state=0)\n",
    "\n",
    "    scores = cross_val_score(bag_clf, X, y, cv=cvKFold)\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    base_dt = DecisionTreeClassifier(random_state=0, criterion='entropy', max_depth=max_depth)\n",
    "    \n",
    "    ada_clf = AdaBoostClassifier(estimator=base_dt,\n",
    "                                 n_estimators=n_estimators,\n",
    "                                 learning_rate=learning_rate,\n",
    "                                 random_state=0)\n",
    "    \n",
    "    scores = cross_val_score(ada_clf, X, y, cv=cvKFold)\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    \n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "                                          learning_rate=learning_rate,\n",
    "                                          random_state=0)\n",
    "    \n",
    "    scores = cross_val_score(gb_clf, X, y, cv=cvKFold)\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy: 0.9386\n",
      "NB average cross-validation accuracy: 0.9264\n",
      "DT average cross-validation accuracy: 0.9179\n",
      "Bagging average cross-validation accuracy: 0.9414\n",
      "AdaBoost average cross-validation accuracy: 0.9250\n",
      "GB average cross-validation accuracy: 0.9300\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: {:.4f}\".format(logregClassifier(X, y)))\n",
    "print(\"NB average cross-validation accuracy: {:.4f}\".format(nbClassifier(X, y)))\n",
    "print(\"DT average cross-validation accuracy: {:.4f}\".format(dtClassifier(X, y)))\n",
    "print(\"Bagging average cross-validation accuracy: {:.4f}\".format(bagDTClassifier(X, y, bag_n_estimators, bag_max_samples, bag_max_depth)))\n",
    "print(\"AdaBoost average cross-validation accuracy: {:.4f}\".format(adaDTClassifier(X, y, ada_n_estimators, ada_learning_rate, ada_bag_max_depth)))\n",
    "print(\"GB average cross-validation accuracy: {:.4f}\".format(gbClassifier(X, y, gb_n_estimators, gb_learning_rate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7, 9]\n",
    "p = [1, 2]\n",
    "\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_neighbors': k,\n",
    "        'p': p\n",
    "    }\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    # Conduct grid search\n",
    "    grid = GridSearchCV(estimator=knn, param_grid=param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = grid.best_params_\n",
    "    best_cv_accuracy = grid.best_score_\n",
    "    \n",
    "    test_accuracy = grid.score(X_test, y_test)\n",
    "    \n",
    "    return best_params, best_cv_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5] \n",
    "gamma = [0.01, 0.1, 1, 10]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 5],\n",
    "        'gamma': [0.01, 0.1, 1, 10]\n",
    "    }\n",
    "    \n",
    "    svm = SVC(kernel='rbf', random_state=0)\n",
    "    \n",
    "    grid = GridSearchCV(estimator=svm, param_grid=param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = grid.best_params_\n",
    "    best_cv_accuracy = grid.best_score_\n",
    "    \n",
    "    test_accuracy = grid.score(X_test, y_test)\n",
    "    \n",
    "    return best_params, best_cv_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 30, 60, 100],\n",
    "        'max_leaf_nodes': [6, 12]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0)\n",
    "    \n",
    "    grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cvKFold)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = grid.best_params_\n",
    "    best_cv_accuracy = grid.best_score_\n",
    "    \n",
    "    y_pred = grid.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return best_params, best_cv_accuracy, test_accuracy, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 9\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9390\n",
      "KNN test set accuracy: 0.9314\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 1.0000\n",
      "SVM cross-validation accuracy: 0.9457\n",
      "SVM test set accuracy: 0.9343\n",
      "\n",
      "RF best n_estimators: 30\n",
      "RF best max_leaf_nodes: 12\n",
      "RF cross-validation accuracy: 0.9390\n",
      "RF test set accuracy: 0.9371\n",
      "RF test set macro average F1: 0.9355\n",
      "RF test set weighted average F1: 0.9370\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_params_KNN, best_cv_acc_KNN, test_acc_KNN = bestKNNClassifier(X, y)\n",
    "best_params_SVM, best_cv_acc_SVM, test_acc_SVM = bestSVMClassifier(X, y)\n",
    "best_params_RF, best_cv_acc_RF, test_acc_RF, macro_f1, weighted_f1 = bestRFClassifier(X, y)\n",
    "# Results of KNN\n",
    "print(\"KNN best k: {}\".format(best_params_KNN['n_neighbors']))\n",
    "print(\"KNN best p: {}\".format(best_params_KNN['p']))\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(best_cv_acc_KNN))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(test_acc_KNN))\n",
    "print()\n",
    "\n",
    "# Reults of SVM\n",
    "print(\"SVM best C: {:.4f}\".format(best_params_SVM['C']))\n",
    "print(\"SVM best gamma: {:.4f}\".format(best_params_SVM['gamma']))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(best_cv_acc_SVM))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(test_acc_SVM))\n",
    "print()\n",
    "\n",
    "#Results of RF\n",
    "print(\"RF best n_estimators: {}\".format(best_params_RF['n_estimators']))\n",
    "print(\"RF best max_leaf_nodes: {}\".format(best_params_RF['max_leaf_nodes']))\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(best_cv_acc_RF))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(test_acc_RF))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(macro_f1))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(weighted_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write one paragraph describing the most important thing that you have learned throughout this assignment.\n",
    "##### Student 1: When I finished this assignment, I realized something I hadn’t noticed before. Pre-processing data and choosing the right parameters is really important. Before starting, I was mostly focused on finding the best algorithm, assuming that a better method would yield better results. However, after dealing with missing values, adjusting class labels, completing normalization, and finding the best parameters through grid search, I saw that these small steps actually improved my results significantly,sometimes even more than using advanced or complicated algorithms. From now on, I will pay more attention to cleaning data and fine-tuning settings, rather than just relying on selecting a sophisticated algorithm. This experience taught me that a combination of proper data preparation and parameter optimization is key to achieving optimal performance.\n",
    "\n",
    "##### Student 2:Through this assignment, we went through a complete machine learning pipeline — from the initial stage of data cleaning, to applying different classification models, and finally using cross-validation to evaluate model performance and tune hyperparameters. This process helped us gain a deeper understanding of machine learning. Through hands-on implementation, we observed that even when working on the same task, using different models or different parameters can lead to significantly different outcomes. My specific responsibility was data cleaning and implementing and evaluating the performance of Bagging, AdaBoost, and Gradient Boosting models. By completing this part, I gained a deeper understanding of ensemble methods. I now better understand how bootstrap aggregation and boosting manipulate the training data, and I also learned how bagging, AdaBoost, and Gradient Boosting are implemented in code. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
